{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "Find the effective dimensionality of the dataset using the PCA which explains the variation between individuals.\n",
    "According to `notebooks/models/3_dim_reduction_autoencoder.ipynb` 95 PCA components are needed to explain 98% of the variance.\n",
    "\n",
    "In [Hoernquist et. al 2002](https://www.ncbi.nlm.nih.gov/pubmed/12069725), they suggest the following approach: add Gaussian Noise to the data and than compute the correlation of the PCA of data with and without noise, for different number of samples.\n",
    "\n",
    "\"_When  we  compare  disturbed  trajectories  with\n",
    "undisturbed,  it  becomes  clear  that  much  of  the\n",
    "variation  of  the  gene  expression  matrix  is  due  to\n",
    "noise.  This  means  that  the  true  dimensionality  of\n",
    "a  system  is  even  less  than  the  principal  values\n",
    "suggest.  However,  from  real  data  with  noise  it  is\n",
    "hard  to  tell  the  number  of  information-carrying\n",
    "dimensions. One way to get a rough estimate is to\n",
    "consider  the  correlation  between  the  different  pc-\n",
    "spaces  obtained  by  analyzing  one  single  trajectory,  but  with  different  Gaussian  noise  attached to  each  measured\n",
    "/ sampled  point.  This  noise  has zero  mean  value  and  standard  deviation\n",
    "(referred  to  as noise level)._\"\n",
    "\n",
    "### TODO\n",
    "1. Generate noise following the Gaussian distribution\n",
    "2. Apply noise over the gene expression data\n",
    "\n",
    "### Conclusions\n",
    "For 25 PCA components the model stays robust enough given the added noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Generate noise following the Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def generate_noise_gaussian(mu = 0, sigma = 0.1, data_shape = [1000, 1000]):\n",
    "    noise = np.random.normal(mu, sigma, data_shape) \n",
    "    return noise\n",
    "\n",
    "def add_noise(X, mu, sigma):    \n",
    "    return X + generate_noise_gaussian(mu, sigma, data_shape = [X.shape[0], X.shape[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Apply noise over the gene expression data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "def subsample(X, size=10):\n",
    "    indexes = np.random.choice(X.shape[0], size)\n",
    "    return X[indexes, :]\n",
    "\n",
    "def compute_corr(X, X_noise):\n",
    "    corr = [np.corrcoef(X[:,i], X_noise[:,i])[0][1] for i in range(X.shape[1])]\n",
    "    return np.median(corr)\n",
    "#     return np.corrcoef(X.flatten(), X_noise.flatten())[0][1]\n",
    "\n",
    "def compute_effective_dim_pca(X):\n",
    "    \n",
    "    df_results = pd.DataFrame(columns=['corr','n_comp','pop_size','sigma'])\n",
    "    \n",
    "    for iteration in range(10):\n",
    "        # variable size of observations\n",
    "        for s in range(100, X.shape[0], 50):\n",
    "\n",
    "            X_subset = subsample(X, size=s)\n",
    "\n",
    "            # variable size of gaussian variance\n",
    "            for sigma in [0.05, 0.10, 0.15, 0.20]:\n",
    "\n",
    "                X_subset_noise = add_noise(X_subset, 0, sigma)\n",
    "\n",
    "                # variable size of PCA comp\n",
    "                for pca_comp in range(1, 101, 10):\n",
    "\n",
    "                    X_pca = PCA(n_components=pca_comp).fit_transform(X_subset)\n",
    "                    X_pca_noise = PCA(n_components=pca_comp).fit_transform(X_subset_noise)   \n",
    "\n",
    "                    corr = compute_corr(X_pca, X_pca_noise)                \n",
    "                    df_results = df_results.append({'corr': corr,\n",
    "                                                    'n_comp': pca_comp,\n",
    "                                                    'pop_size': s,\n",
    "                                                    'sigma': sigma}, ignore_index=True)\n",
    "            print(f'Done...pop_size={s}; n_comp={pca_comp}; sigma:{sigma}; corr={corr}')\n",
    "        \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from rep import preprocessing_new as p\n",
    "\n",
    "file = os.path.join(os.readlink(os.path.join(\"..\",\"..\",\"data\")),\"processed\",\"gtex\",\"recount\",\"recount_gtex_logratios.h5ad\")\n",
    "gtex = p.RepAnnData.read_h5ad(file)\n",
    "blood = gtex[gtex.samples['Tissue'] == 'Whole Blood']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(429, 19932)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blood.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done...pop_size=100; n_comp=91; sigma:0.2; corr=0.7396705463299397\n",
      "Done...pop_size=150; n_comp=91; sigma:0.2; corr=0.7651085723992201\n",
      "Done...pop_size=200; n_comp=91; sigma:0.2; corr=0.6791335232199706\n",
      "Done...pop_size=250; n_comp=91; sigma:0.2; corr=0.9119298179227409\n",
      "Done...pop_size=300; n_comp=91; sigma:0.2; corr=0.9439124845055735\n",
      "Done...pop_size=350; n_comp=91; sigma:0.2; corr=0.8531398755714464\n",
      "Done...pop_size=400; n_comp=91; sigma:0.2; corr=0.9302155872862768\n",
      "Done...pop_size=100; n_comp=91; sigma:0.2; corr=0.6497845430568674\n",
      "Done...pop_size=150; n_comp=91; sigma:0.2; corr=0.5950391425516716\n",
      "Done...pop_size=200; n_comp=91; sigma:0.2; corr=0.8445890358070748\n",
      "Done...pop_size=250; n_comp=91; sigma:0.2; corr=0.9261974529769073\n",
      "Done...pop_size=300; n_comp=91; sigma:0.2; corr=0.8938787798536996\n",
      "Done...pop_size=350; n_comp=91; sigma:0.2; corr=0.8551737827323812\n",
      "Done...pop_size=400; n_comp=91; sigma:0.2; corr=0.9529786007007178\n",
      "Done...pop_size=100; n_comp=91; sigma:0.2; corr=0.7424128749676591\n",
      "Done...pop_size=150; n_comp=91; sigma:0.2; corr=0.6565060846328252\n",
      "Done...pop_size=200; n_comp=91; sigma:0.2; corr=0.8813209609530599\n",
      "Done...pop_size=250; n_comp=91; sigma:0.2; corr=0.878481538400748\n",
      "Done...pop_size=300; n_comp=91; sigma:0.2; corr=0.9151182712128197\n",
      "Done...pop_size=350; n_comp=91; sigma:0.2; corr=0.9125097394212919\n",
      "Done...pop_size=400; n_comp=91; sigma:0.2; corr=0.7833769256700015\n",
      "Done...pop_size=100; n_comp=91; sigma:0.2; corr=0.7030844294890154\n",
      "Done...pop_size=150; n_comp=91; sigma:0.2; corr=0.6831678452319222\n",
      "Done...pop_size=200; n_comp=91; sigma:0.2; corr=0.8004587484237177\n",
      "Done...pop_size=250; n_comp=91; sigma:0.2; corr=0.7277357466223684\n",
      "Done...pop_size=300; n_comp=91; sigma:0.2; corr=0.8759082342952847\n",
      "Done...pop_size=350; n_comp=91; sigma:0.2; corr=0.9272211474085819\n",
      "Done...pop_size=400; n_comp=91; sigma:0.2; corr=0.8674205444094321\n",
      "Done...pop_size=100; n_comp=91; sigma:0.2; corr=0.5997043621641618\n",
      "Done...pop_size=150; n_comp=91; sigma:0.2; corr=0.7496691987947197\n",
      "Done...pop_size=200; n_comp=91; sigma:0.2; corr=0.8929241554017477\n",
      "Done...pop_size=250; n_comp=91; sigma:0.2; corr=0.9268377099048057\n",
      "Done...pop_size=300; n_comp=91; sigma:0.2; corr=0.8827963486728988\n",
      "Done...pop_size=350; n_comp=91; sigma:0.2; corr=0.9277174231618989\n"
     ]
    }
   ],
   "source": [
    "eff_dim_data = compute_effective_dim_pca(np.array(blood.X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_dim_data.to_csv(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from plotnine import *\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "%matplotlib inline\n",
    "df = eff_dim_data.groupby(['n_comp', 'pop_size','sigma']).mean()\n",
    "\n",
    "fig = (\n",
    "        ggplot(df, aes(x='n_comp', y='corr',colour='factor(pop_size)')) + \n",
    "        geom_line() +\n",
    "        geom_point() + \n",
    "        theme_bw() + \n",
    "        facet_wrap('~sigma', ncol = 2) + \n",
    "        ggtitle('Effective dimensionality of PCA (using the correlation between data and data+noise)'))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each data point represents the correlation between the data and data+noise (blood log ratios) with respect of the number of components to compute the PCA. Each point represents the mean value over the 10 iterations of repeating the experiment. The data is subsampled ($[100,400]$ individuals). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rep)",
   "language": "python",
   "name": "rep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
