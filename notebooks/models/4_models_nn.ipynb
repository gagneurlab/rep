{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "Implement a neural network model using the individual and gene effect information.\n",
    "\n",
    "### TODO \n",
    " - 1. Autoencoder for individual effect\n",
    "\n",
    "### Conclusions\n",
    " - 1. For a model with one single layer 95 or 64 nodes does not make any difference. Using Huber-loss insead of mean squared error also does not make any difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mada/anaconda3/envs/rep/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import comet_ml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Add, Dropout, Embedding, Conv2D, Flatten, Conv1D, MaxPooling1D, LeakyReLU, Lambda\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.losses import *\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.regularizers import *\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "import rep.preprocessing_new as prep\n",
    "import rep.datasets as d\n",
    "import rep.models as m\n",
    "import rep.metrics as mt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%autoreload 1\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "\n",
    "# import keras\n",
    "# from keras import backend as K\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import backend as K\n",
    "# from tensorflow.keras.layers import *\n",
    "\n",
    "# from tensorflow.keras.models import Model, Sequential\n",
    "# from tensorflow.keras.losses import *\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from tensorflow.keras.regularizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto( device_count = {'GPU': 1, 'CPU': 4} ) \n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 8457642798632358223\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 10111682728968969294\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16082512859742057091\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3273981952\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 9285727182868592132\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.readlink(os.path.join(\"..\",\"..\",\"data\"))\n",
    "path = os.path.join(data_path,\"processed\",\"gtex\")\n",
    "x_inputs_h5 = os.path.join(path,\"X_inputs_pc_onlyblood.h5\")\n",
    "y_targets_h5 = os.path.join(path,\"Y_targets_pc_onlyblood.h5\")\n",
    "train_raw, valid_raw = d.rep_blood_expression(x_inputs_h5, y_targets_h5, label=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter train and valid by tissue\n",
    "train = train_raw.filter_by(from_tissue='Whole Blood',to_tissue='Muscle - Skeletal')\n",
    "valid = valid_raw.filter_by(from_tissue='Whole Blood',to_tissue='Muscle - Skeletal')\n",
    "del train_raw, valid_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((220, 19932), (220, 19932), (220, 11), (19932, 3))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.inputs.shape, train.targets.shape, train.metadata.shape, train.features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive plot of the loss\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class PlotLosses(keras.callbacks.Callback):\n",
    "    \n",
    "#     def __init__(self,start_epoch=1):\n",
    "#         self.start_epoch = start_epoch\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "#         print(epoch)\n",
    "#         if epoch >= self.start_epoch:\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.i += 1\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.x, self.losses, label=\"loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "plot_losses = PlotLosses()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Autoencoder for individual effect\n",
    "The AE should perform similar to the PCA (idealy even better)<br/>\n",
    "PCA performance for the individual effect (mean_spearmanr = 0.78, median_spearmanr = 0.93 (ncomp = 95)\n",
    "\n",
    "Model 1 hidden layer with 95 nodes: mean_spearmanr = 0.77, median_spearmanr = 0.93 <br/>\n",
    "Model 1 hidden layer with 95 nodes + huber_loss: mean_spearmanr = 0.77, median_spearmanr = 0.93 <br/>\n",
    "Model 1 hidden layer with 64 nodes: mean_spearmanr = 0.77, median_spearmanr = 0.93 <br/>\n",
    "Model 1 hidden layer with 32 nodes: mean_spearmanr = 0.77, median_spearmanr = 0.92 <br/>\n",
    "\n",
    "Adding more layers / Dropout or activation function does not help improving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.utils.validation import check_array as check_arrays\n",
    "\n",
    "# loss\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return K.mean(K.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def huber_loss(y_true, y_pred, clip_delta=1):\n",
    "    error = y_true - y_pred\n",
    "    cond  = tf.keras.backend.abs(error) < clip_delta\n",
    "\n",
    "    squared_loss = 0.5 * tf.keras.backend.square(error)\n",
    "    linear_loss  = clip_delta * (tf.keras.backend.abs(error) - 0.5 * clip_delta)\n",
    "\n",
    "    return tf.where(cond, squared_loss, linear_loss)\n",
    "\n",
    "\n",
    "# optimizer\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, decay=0.0000001)\n",
    "\n",
    "# autoencoder\n",
    "code_size = 32\n",
    "ae = Sequential([   \n",
    "    Dense(128, input_shape=(train.inputs.shape[1],)),\n",
    "    Dense(code_size),\n",
    "    Dense(128),\n",
    "    Dense(train.inputs.shape[1])\n",
    "])\n",
    "ae.compile(optimizer = adam, \n",
    "           loss = 'mean_squared_error')\n",
    "#            loss = huber_loss)\n",
    "\n",
    "\n",
    "ae.fit(train.inputs,\n",
    "       train.inputs,\n",
    "       epochs=500,\n",
    "       batch_size=16,\n",
    "       validation_data=(valid.inputs, valid.inputs),\n",
    "       verbose=0,\n",
    "       callbacks=[EarlyStopping(monitor='val_loss', min_delta=1e-5, patience=10),\n",
    "                 plot_losses])\n",
    "\n",
    "recovered_data = ae.predict(valid.inputs)\n",
    "eval_ae = mt.rep_metric(valid.inputs, recovered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.7748012330096412,\n",
       " 'min': -0.32335198926422715,\n",
       " 'p10': 0.2714703348359809,\n",
       " 'p25': 0.682854766264811,\n",
       " 'median': 0.9232028716269403,\n",
       " 'p75': 0.9681769996956242,\n",
       " 'p90': 0.9802680604972868,\n",
       " 'max': 0.9939544695991687}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_ae['spearmanr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Neural network - implement gene expression imputation from blood to muscle tissue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Implement a network with one layer to predict from blood to muscle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.32708409937405153,\n",
       " 'min': -0.4964952687755446,\n",
       " 'p10': 0.015261935930561157,\n",
       " 'p25': 0.16836818324962177,\n",
       " 'median': 0.34903650372417216,\n",
       " 'p75': 0.49716618739487645,\n",
       " 'p90': 0.60299649450203,\n",
       " 'max': 0.8727828040531936}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden = 64\n",
    "nn = Sequential([\n",
    "    Dense(hidden, input_shape=(train.inputs.shape[1],)),\n",
    "    Dense(train.inputs.shape[1])\n",
    "])\n",
    "nn.compile(optimizer = adam, \n",
    "           loss = 'mean_squared_error')\n",
    "nn.reset_states()\n",
    "nn.fit(train.inputs,\n",
    "       train.targets,\n",
    "       epochs=500,\n",
    "       batch_size=16,\n",
    "       validation_data=(valid.inputs, valid.targets),\n",
    "       verbose=0,\n",
    "       callbacks=[EarlyStopping(monitor='val_loss', min_delta=0.000001, patience=5)])\n",
    "recovered_data = nn.predict(valid.inputs)\n",
    "eval_nn = mt.rep_metric(valid.targets, recovered_data)\n",
    "eval_nn['spearmanr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Add layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train.inputs[:,:10000].reshape(-1,100,100)\n",
    "y_train = train.targets[:,:10000]\n",
    "x_valid = valid.inputs[:,:10000].reshape(-1,100,100)\n",
    "y_valid = valid.targets[:,:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "# optimizer\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, decay=0.0000001)\n",
    "\n",
    "def simple_model():\n",
    "    \n",
    "    hidden = 1024\n",
    "    hidden_2 = 256\n",
    "    hidden_3 = 64\n",
    "    hidden_4 = 32\n",
    "    \n",
    "    nn_aux = Sequential([\n",
    "        Dense(hidden, input_shape=(train.inputs.shape[1],)),\n",
    "        Dense(hidden_2),\n",
    "        Dense(hidden_3),\n",
    "        Dense(hidden_4),\n",
    "        Dense(train.inputs.shape[1])\n",
    "    ])\n",
    "    nn_aux.compile(optimizer = adam, \n",
    "               loss = 'mean_squared_error')\n",
    "\n",
    "    return nn_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = simple_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.29230391734801814,\n",
       " 'min': -0.40162066413046044,\n",
       " 'p10': -0.008554778273798682,\n",
       " 'p25': 0.12727561228072834,\n",
       " 'median': 0.30400512187068746,\n",
       " 'p75': 0.457812926409061,\n",
       " 'p90': 0.5804582380302498,\n",
       " 'max': 0.861583996353003}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.reset_states()\n",
    "nn.fit(train.inputs,\n",
    "       train.targets,\n",
    "       epochs=500,\n",
    "       batch_size=16,\n",
    "       validation_data=(valid.inputs, valid.targets),\n",
    "       verbose=0,\n",
    "       callbacks=[EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=5)])\n",
    "recovered_data = nn.predict(valid.inputs)\n",
    "eval_nn = mt.rep_metric(valid.targets, recovered_data)\n",
    "eval_nn['spearmanr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.save(\"simple_layer_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Network \n",
    "- contains the AE hidden layer of the individual variation as first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "hidden = 128\n",
    "hidden_2 = 64\n",
    "hidden_3 = 32\n",
    "\n",
    "x = torch.tensor(train.inputs)\n",
    "y = torch.tensor(train.targets)\n",
    "model = nn.Sequential(\n",
    "                    nn.Linear(train.inputs.shape[1],hidden),\n",
    "                    nn.Conv1d(hidden,1,3,stride=1),\n",
    "                    nn.Conv1d(10,10,3,stride=2,dilation=2),\n",
    "                    nn.Conv1d(10,10,3,stride=4,dilation=4)\n",
    "#     ,\n",
    "#                     Flatten(),\n",
    "#                     nn.Linear(hidden, train.inputs.shape[1])\n",
    "                    )\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=0.0000001)\n",
    "\n",
    "\n",
    "for epoch in range(500):\n",
    "    # Forward Propagation\n",
    "    y_pred = model(x)\n",
    "    print(y_pred.shape)\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        print('epoch: ', epoch,' loss: ', loss.item())\n",
    "    \n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # perform a backward pass (backpropagation)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the parameters\n",
    "    optimizer.step()\n",
    "\n",
    "x_valid = torch.tensor(valid.inputs)\n",
    "y_valid = torch.tensor(valid.targets)\n",
    "\n",
    "y_pred = model(x_valid)\n",
    "eval_nn = mt.rep_metric(valid.targets, y_pred.detach().numpy())\n",
    "eval_nn['spearmanr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pycuda.driver as cuda\n",
    "\n",
    "cuda.init()\n",
    "\n",
    "## Get Id of default device\n",
    "torch.cuda.current_device()\n",
    "# 0\n",
    "cuda.Device(0).name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220, 19932)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = train.inputs[:,:18000].reshape(-1,200,90)\n",
    "# y_train = train.targets[:,:18000]\n",
    "# x_valid = valid.inputs[:,:18000].reshape(-1,200,90)\n",
    "# y_valid = valid.targets[:,:18000]\n",
    "\n",
    "x_train = train.inputs[:,:18000].reshape(-1,200,90)\n",
    "y_train = train.targets\n",
    "x_valid = valid.inputs[:,:18000].reshape(-1,200,90)\n",
    "y_valid = valid.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, decay=0.00000001)\n",
    "\n",
    "def model_conv(input_size1, input_size2):\n",
    "    \n",
    "    \n",
    "    # first submodel\n",
    "    input1 = keras.layers.Input(shape=(input_size1[0],input_size1[1],1))\n",
    "        \n",
    "    hidden3 = keras.layers.Conv2D(7, (3,3), strides=1, padding='valid')(input1)\n",
    "    hidden3 = keras.layers.Conv2D(7, (3,3), dilation_rate = 2, padding='valid')(hidden3)\n",
    "#     hidden3 = keras.layers.Conv2D(7, (3,3), dilation_rate = 4, padding='valid')(hidden3)\n",
    "\n",
    "    flat = keras.layers.Flatten()(hidden3)\n",
    "    \n",
    "    outh = keras.layers.Dropout(rate=0.2)(flat)\n",
    "    outh = keras.layers.Dense(32)(outh)\n",
    "       \n",
    "    outh = keras.layers.Dropout(rate=0.2)(outh)\n",
    "    outh = keras.layers.Dense(64)(outh)\n",
    "   \n",
    "    outh = keras.layers.Dropout(rate=0.2)(outh)\n",
    "    outh1 = keras.layers.Dense(128)(outh)\n",
    "\n",
    "#     outh1 = keras.layers.Dropout(0.2)(outh)\n",
    "    \n",
    "#     outh1 = keras.layers.Dense(input_size[0]*input_size[1])(outh)\n",
    "    \n",
    "#     nn_aux = keras.models.Model(inputs=input1, outputs=outh)\n",
    "#     nn_aux.compile(optimizer = adam, \n",
    "#                loss = 'mean_squared_error')\n",
    "\n",
    "    \n",
    "    # second submodel\n",
    "    input2 = keras.layers.Input(shape=(input_size2,))\n",
    "    print(input2.shape)\n",
    "    outh2 = keras.layers.Dense(64)(input2)\n",
    "    outh2 = keras.layers.Dropout(rate=0.25)(outh2)\n",
    "    outh2 = keras.layers.Dense(32)(outh2)\n",
    "    outh2 = keras.layers.Dropout(rate=0.25)(outh2)\n",
    "\n",
    "\n",
    "    print(outh2.shape)\n",
    "#     outh1 = keras.layers.Reshape(target_shape=(1,128))(outh1)\n",
    "#     outh2 = keras.layers.Reshape(target_shape=(1,32))(outh2)\n",
    "#     concat = keras.layers.concatenate([outh1, outh2])\n",
    "#     print(concat.shape)\n",
    "#     concat = K.squeeze(concat,axis=1)\n",
    "#     print(concat.shape)\n",
    "#     outh3 = keras.layers.Dense(input_size[0]*input_size[1])(concat)\n",
    "#     print(outh3.shape)\n",
    "#     print(input1.shape)\n",
    "#     print(input2.shape)\n",
    "#     print(outh3.shape)\n",
    "    outh3 = keras.layers.Dense(input_size2)(outh2)\n",
    "    print(outh3.shape)\n",
    "    nn_aux = keras.models.Model(inputs=[input1,input2], outputs=[outh3])\n",
    "    nn_aux.compile(optimizer = adam, \n",
    "               loss = 'mean_squared_error')\n",
    "\n",
    "    \n",
    "    \n",
    "    return nn_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "model_conv() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-eac03a951d4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m nn.fit(x=[K.expand_dims(x_train),train.inputs],\n\u001b[1;32m      4\u001b[0m        \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m        \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: model_conv() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "nn = model_conv((x_train.shape[1],x_train.shape[2]), train.inputs.shape[1])\n",
    "nn.reset_states()\n",
    "nn.fit(x=[K.expand_dims(x_train),train.inputs],\n",
    "       y=y_train,\n",
    "       epochs=500,\n",
    "#        batch_size=16,\n",
    "       steps_per_epoch=16,\n",
    "       validation_steps=1,\n",
    "       validation_data=([K.expand_dims(x_valid),valid.inputs], y_valid),\n",
    "       verbose=0,\n",
    "       callbacks=[\n",
    "           EarlyStopping(monitor='val_loss', min_delta=1e-5, patience=10)\n",
    "           , \n",
    "           plot_losses\n",
    "       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': 0.3637025457788201,\n",
       " 'min': -0.4228279308306054,\n",
       " 'p10': 0.06465575036333858,\n",
       " 'p25': 0.223556140680859,\n",
       " 'median': 0.3907617708395499,\n",
       " 'p75': 0.5231454270948735,\n",
       " 'p90': 0.6130932639561851,\n",
       " 'max': 0.8423455572579237}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = nn.predict([K.expand_dims(x_valid),valid.inputs],steps=1)\n",
    "eval_nn = mt.rep_metric(y_valid, y_pred)\n",
    "eval_nn['spearmanr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "adam = Adam(lr=0.00005, beta_1=0.9, beta_2=0.999, decay=0.0000001)\n",
    "\n",
    "def model_conv(input_size_):\n",
    "    \n",
    "#     code_size = 64\n",
    "#     inputLength = 64\n",
    "#     baseNetwork = Sequential()\n",
    "# #     baseNetwork.add(Dense(code_size))\n",
    "#     baseNetwork.add(Embedding(input_dim=100, output_dim=80, input_length=input_size_))\n",
    "#     baseNetwork.add(Conv1D(10, 20, strides=1, padding='valid'))\n",
    "#     baseNetwork.add(LeakyReLU(0.5))\n",
    "#     baseNetwork.add(Dropout(0.1))\n",
    "\n",
    "#     baseNetwork.add(Conv1D(10, 20, dilation_rate=2, padding='valid'))\n",
    "#     baseNetwork.add(LeakyReLU(0.5))\n",
    "#     baseNetwork.add(Dropout(0.1))\n",
    "\n",
    "#     baseNetwork.add(Conv1D(10, 20, dilation_rate=4, padding='valid'))\n",
    "#     baseNetwork.add(LeakyReLU(0.5))\n",
    "#     baseNetwork.add(Dropout(0.1))\n",
    "\n",
    "#     baseNetwork.add(Flatten())\n",
    "\n",
    "#     baseNetwork.add(Dense(128))\n",
    "# #     baseNetwork.add(Dropout(0.1))\n",
    "#     baseNetwork.add(Dense(code_size))\n",
    "#     baseNetwork.add(Dense(code_size))\n",
    "# #     baseNetwork.add(Dropout(0.1))\n",
    "#     baseNetwork.add(Dense(32))\n",
    "#     baseNetwork.add(Dropout(0.1))\n",
    "# #     baseNetwork.add(Dense(16))\n",
    "# #     baseNetwork.add(Dense(16))\n",
    "# #     baseNetwork.add(Dropout(0.1))\n",
    "#     baseNetwork.add(Dense(input_size_))\n",
    "\n",
    "    \n",
    "    input1 = keras.layers.Input(shape=(input_size_,))\n",
    "    emb = keras.layers.Embedding(input_dim=10, output_dim=128, input_length=input_size_)(input1)\n",
    "    flat = keras.layers.Flatten()(emb)\n",
    "    h1 = keras.layers.Dense(128, activation='sigmoid', kernel_regularizer=keras.regularizers.l2(0.005))(flat)\n",
    "    h2 = keras.layers.Dense(64, activation='sigmoid', kernel_regularizer=keras.regularizers.l2(0.005))(h1)\n",
    "    h3 = keras.layers.Dense(32, activation='sigmoid', kernel_regularizer=keras.regularizers.l2(0.005))(h2)\n",
    "    h4 = keras.layers.Dense(64, activation='sigmoid', kernel_regularizer=keras.regularizers.l2(0.005))(h3)    \n",
    "    h5 = keras.layers.Dense(128, activation='sigmoid', kernel_regularizer=keras.regularizers.l2(0.005))(h4)\n",
    "    h6 =  Add()([h1, h5])\n",
    "    out1 = Dropout(0.1)(h6)\n",
    "\n",
    "    out2 = keras.layers.Dense(input_size_)(out1)\n",
    "    baseNetwork = keras.models.Model(inputs=[input1], outputs=[out2])\n",
    "    baseNetwork.compile(optimizer = adam, \n",
    "               loss = 'mean_squared_error')\n",
    "   \n",
    "    \n",
    "    return baseNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = model_conv(train.inputs.shape[1])\n",
    "nn.reset_states()\n",
    "nn.fit(train.inputs,\n",
    "       train.targets,\n",
    "       epochs=500,\n",
    "       batch_size=16,\n",
    "#        steps_per_epoch=16,\n",
    "#        validation_steps=1,\n",
    "       validation_data=(valid.inputs, valid.targets),\n",
    "       verbose=0,\n",
    "       callbacks=[\n",
    "           EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=10), \n",
    "           plot_losses\n",
    "       ])\n",
    "y_pred = nn.predict(valid.inputs)\n",
    "eval_nn = mt.rep_metric(valid.targets, y_pred)\n",
    "eval_nn['spearmanr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden = 256\n",
    "# hidden_2 = 128\n",
    "# hidden_3 = 32\n",
    "# hidden_4 = 16\n",
    "# # hidden_2 = 1024\n",
    "# nn = Sequential([\n",
    "#     Dense(code_size, input_shape=(train.inputs.shape[1],), trainable=False),\n",
    "#     Dense(hidden_2),\n",
    "#     Dense(hidden_3),\n",
    "#     Dense(hidden_4),\n",
    "#     Dense(train.inputs.shape[1])\n",
    "# ])\n",
    "    \n",
    "    \n",
    "#     Dropout(0.1),\n",
    "#     Dense(hidden_2),\n",
    "#     Dropout(0.1),\n",
    "#     Conv1D(filters=16, kernel_size=(3), activation='relu', input_shape=(hidden_2,1)),\n",
    "#     MaxPooling1D(pool_size=2),\n",
    "#     Flatten(),\n",
    "#     Dense(train.inputs.shape[1])\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/rep/lib/python3.6/site-packages/keras/regularizers.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?keras.regularizers.l1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rep)",
   "language": "python",
   "name": "rep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
