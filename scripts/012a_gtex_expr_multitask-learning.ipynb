{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import collections\n",
    "import random\n",
    "import math\n",
    "import numpy.random as nr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import joblib\n",
    "\n",
    "import xarray as xr\n",
    "import dask\n",
    "import dask.dataframe as ddf\n",
    "import dask.array as da\n",
    "import zarr\n",
    "\n",
    "# set default scheduler to threaded\n",
    "dask.config.set(scheduler='threads')\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import scanpy.api as sc\n",
    "import anndata as ad\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import plotnine as pn\n",
    "\n",
    "## init plotly\n",
    "# from plotly.offline import iplot, init_notebook_mode\n",
    "# init_notebook_mode(connected=True)\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe_connected'\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "\n",
    "import datashader as ds\n",
    "import holoviews as hv\n",
    "import holoviews.operation.datashader as hd\n",
    "hd.shade.cmap=[\"lightblue\", \"darkblue\"]\n",
    "hv.extension(\"bokeh\", \"matplotlib\")\n",
    "import hvplot\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from scipy import (\n",
    "    stats as scistats,\n",
    "    special as scispecial,\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!env|grep -i cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.cache import Cache\n",
    "cache = Cache(8e9)  # Leverage eight gigabytes of memory\n",
    "cache.register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.expanduser(\"~/Projects/REP/rep\"))\n",
    "import rep.random as rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR=\"/s/project/rep/cache/\"\n",
    "RAW_DATA_DIR=\"/s/project/rep/raw/\"\n",
    "PROCESSED_DATA_DIR=\"/s/project/rep/processed/\"\n",
    "MODEL_DIR=os.path.join(PROCESSED_DATA_DIR, \"training_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_MODEL_DIR=os.path.join(MODEL_DIR, \"expr_multitask\")\n",
    "\n",
    "if not os.path.exists(CURRENT_MODEL_DIR):\n",
    "    os.mkdir(CURRENT_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrds = xr.open_zarr(os.path.join(PROCESSED_DATA_DIR, \"gtex/OUTRIDER/xarray_unstacked.zarr\"))\n",
    "xrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_xrds = xrds.stack(observations=[\"individual\", \"genes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blood = stacked_xrds.sel(subtissue=\"Whole_Blood\")\n",
    "blood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Tuple, List\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "def concat_by_axis(\n",
    "    darrs: Union[List[xr.DataArray], Tuple[xr.DataArray]],\n",
    "    dims: Union[List[str], Tuple[str]],\n",
    "    axis: int = None,\n",
    "    drop_coords=True,\n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Concat arrays along some axis similar to `np.concatenate`. Automatically renames the dimensions to `dims`.\n",
    "    Please note that this renaming happens by the axis position, therefore make sure to transpose all arrays\n",
    "    to the correct dimension order.\n",
    "\n",
    "    :param darrs: List or tuple of xr.DataArrays\n",
    "    :param dims: The dimension names of the resulting array. Renames axes where necessary.\n",
    "    :param axis: The axis which should be concatenated along\n",
    "    :param kwargs: Additional arguments which will be passed to `xr.concat()`\n",
    "    :return: Concatenated xr.DataArray with dimensions `dim`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get depth of nested lists. Assumes `darrs` is correctly formatted as list of lists.\n",
    "    if axis is None:\n",
    "        axis = 0\n",
    "        l = darrs\n",
    "        # while l is a list or tuple and contains elements:\n",
    "        while isinstance(l, List) or isinstance(l, Tuple) and l:\n",
    "            # increase depth by one\n",
    "            axis -= 1\n",
    "            l = l[0]\n",
    "        if axis == 0:\n",
    "            raise ValueError(\"`darrs` has to be a (possibly nested) list or tuple of xr.DataArrays!\")\n",
    "\n",
    "    to_concat = list()\n",
    "    for i, da in enumerate(darrs):\n",
    "        # recursive call for nested arrays;\n",
    "        # most inner call should have axis = -1,\n",
    "        # most outer call should have axis = - depth_of_darrs\n",
    "        if isinstance(da, list) or isinstance(da, tuple):\n",
    "            da = concat_axis(da, dims=dims, axis=axis + 1, **kwargs)\n",
    "\n",
    "        if not isinstance(da, xr.DataArray):\n",
    "            raise ValueError(\"Input %d must be a xr.DataArray\" % i)\n",
    "        if len(da.dims) != len(dims):\n",
    "            raise ValueError(\"Input %d must have the same number of dimensions as specified in the `dims` argument!\" % i)\n",
    "\n",
    "        # force-rename dimensions\n",
    "        da = da.rename(dict(zip(da.dims, dims)))\n",
    "        \n",
    "        # remove coordinates\n",
    "        if drop_coords:\n",
    "            da = da.reset_coords(drop=True)\n",
    "\n",
    "        to_concat.append(da)\n",
    "\n",
    "    return xr.concat(to_concat, dim=dims[axis], **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = concat_by_axis([\n",
    "    blood.cdf.expand_dims({\"features\": [\"cdf\"]}, axis=-1), \n",
    "    blood.padj.expand_dims({\"features\": [\"padj\"]}, axis=-1), \n",
    "    blood.hilo_padj.expand_dims({\"features\": [\"hilo_padj\"]}, axis=-1), \n",
    "    blood.missing.expand_dims({\"features\": [\"missing\"]}, axis=-1), \n",
    "], dims=(\"observations\", \"features\"), fill_value=0., coords=\"minimal\")\n",
    "features = features.fillna(0)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_xrds[\"c_features\"] = features\n",
    "stacked_xrds[\"cdf\"] = stacked_xrds[\"cdf\"].fillna(0)\n",
    "stacked_xrds[\"normppf\"] = stacked_xrds[\"normppf\"].fillna(0)\n",
    "stacked_xrds[\"padj\"] = stacked_xrds[\"padj\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_xrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(blood.hilo_padj.values[~ blood.missing.values]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(stacked_xrds.hilo_padj.values[~ stacked_xrds.missing.values]).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into test and train sets\n",
    "testDelim = int(blood.dims[\"observations\"] * 0.8)\n",
    "\n",
    "train = stacked_xrds.isel(observations=slice(None, testDelim))\n",
    "test  = stacked_xrds.isel(observations=slice(testDelim, None))\n",
    "\n",
    "print(\"train:\")\n",
    "print(train)\n",
    "print(\"test:\")\n",
    "print(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input_features = train.dims[\"features\"]\n",
    "n_input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_targets = train.dims[\"subtissue\"]\n",
    "n_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = k.Sequential([\n",
    "#     k.layers.Dense(units=20, activation=\"relu\", input_shape=(n_input_features,)),\n",
    "    k.layers.Dense(units=n_targets, activation=\"linear\", input_shape=(n_input_features,)),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=\"mse\",\n",
    "    metrics=[\"mse\", \"mae\", \"mape\", 'cosine'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x=train.c_features.transpose(\"observations\", \"features\", transpose_coords=False), \n",
    "    y=train.normppf.transpose(\"observations\", \"subtissue\", transpose_coords=False), \n",
    "    batch_size=315230, \n",
    "    shuffle=False,\n",
    "    validation_split=0.1,\n",
    "    epochs=100,\n",
    "    class_weight=train.missing.transpose(\"observations\", \"subtissue\", transpose_coords=False),\n",
    "    callbacks=[\n",
    "        k.callbacks.EarlyStopping(patience=4),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(CURRENT_MODEL_DIR, \"model.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lazy(features, model, feature_dim=\"features\", output_dim=\"subtissue\", output_size=None):\n",
    "    \"\"\"\n",
    "    Predicts using a (Keras-) model with a two-dimensional input and two-dimensional output,\n",
    "    keeps xarray metadata and dask chunks\n",
    "    \"\"\"\n",
    "    if output_size==None:\n",
    "        output_size = model.output.shape[-1].value\n",
    "    \n",
    "    model_predict_lazy = da.gufunc(\n",
    "        model.predict, \n",
    "        signature=\"(features)->(classes)\", \n",
    "        output_dtypes=\"float32\", \n",
    "        output_sizes={\"classes\": output_size}, \n",
    "        allow_rechunk=True, \n",
    "        vectorize=False\n",
    "    )\n",
    "    if isinstance(features, xr.DataArray):\n",
    "        return xr.apply_ufunc(\n",
    "            predict_lazy, features, \n",
    "            kwargs={\"model\": model}, \n",
    "            input_core_dims=[[feature_dim]], \n",
    "            output_core_dims=[[output_dim]], \n",
    "            dask=\"allowed\",\n",
    "        )\n",
    "    else:\n",
    "        return model_predict_lazy(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = predict_lazy(test[\"c_features\"], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save = predicted.reset_coords(drop=True).reset_index(\"observations\").rename(\"predicted\")\n",
    "to_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"size of data to save: %.2f MB\" % (to_save.nbytes/2**20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save.sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with dask.config.set(scheduler='single-threaded'):\n",
    "    to_save.chunk({\"observations\": 2**22 // to_save.sizes[\"subtissue\"]}).to_dataset(name=\"predicted\").to_zarr(os.path.join(CURRENT_MODEL_DIR, \"predicted.zarr\"), mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.any(np.isnan(to_save)).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda-florian3]",
   "language": "python",
   "name": "conda-env-anaconda-florian3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
